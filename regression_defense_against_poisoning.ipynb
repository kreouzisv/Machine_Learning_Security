import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

# Make numpy printouts easier to read.
np.set_printoptions(precision=3, suppress=True)
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers.experimental import preprocessing
import statistics
import tensorflow_probability as tfp
tfd = tfp.distributions
import random

print(tf.__version__)


# In[2]:


def poisson_attack_data_injection(data_set,percent_injection):
    # Get dataset columns
    columns = data_set.columns
    # Determine params of each column
    means = []
    var = []
    for i in columns:
        means.append(np.mean(data_set[i]))
        var.append(np.var(raw_dataset[i]))
    
    # Inject random noise into the variance parameter
    for i in range(len(var)):
        var[i] += tfd.Gamma(10,5).sample(1).numpy()
        
    # Generate poissoned data
    number_of_injections = percent_injection * data_set.shape[0]
    new_data = []
    for j in range(len(columns)):
        new_data.append(pd.DataFrame(abs(tfd.Normal(means[j],var[j]).sample(number_of_injections).numpy())))
    
    # Merge initial dataset with poissoned data
    data = pd.concat(new_data,axis=1)
    data.columns = columns
    frames = [data_set,data]
    poissoned_data_injected_dataset = pd.concat(frames)
        
    return poissoned_data_injected_dataset

def poisson_attack_data_modification(data_set,percent_modification):
    # Select random indeces to modify
    mod_num = data_set.shape[0] * percent_modification
    rand_idx = random.sample(list(range(1, data_set.shape[0])),round(mod_num))
    # Inject random noise into the dataset
    for i in rand_idx:
        data_set.loc[i] += abs(tfd.Normal(0,5).sample(1))
    
    return data_set

def poisson_attack_data_deletion(data_set,percent_deletion):
    # Select random indeces to delete
    mod_num = data_set.shape[0] * percent_deletion
    rand_idx = random.sample(list(range(1, data_set.shape[0])),round(mod_num))
    # Delete observations
    poissoned_data_deletion = data_set.drop(rand_idx,axis=0)
        
    return poissoned_data_deletion


# In[38]:


def simple_regression_tf(train_features,train_labels,loss_fn):
   def plot_loss(history):
       plt.plot(history.history['loss'], label='loss')
       plt.plot(history.history['val_loss'], label='val_loss')
       plt.ylim([0, 10])
       plt.xlabel('Epoch')
       plt.ylabel('Error [MPG]')
       plt.legend()
       plt.grid(True)
       
   # set up a normalizer
   normalizer = preprocessing.Normalization()
   # Adapt it to the train features
   normalizer.adapt(np.array(train_features))
   # Define feature for training
   horsepower = np.array(train_features['Horsepower'])
   horsepower_normalizer = preprocessing.Normalization(input_shape=[1,])
   horsepower_normalizer.adapt(horsepower)

   # Set up the model MPG ~ Horsepower
   horsepower_model = tf.keras.Sequential([
       horsepower_normalizer,
       layers.Dense(units=1)
   ])

   # Compile the model
   horsepower_model.compile(
       optimizer=tf.optimizers.Adam(learning_rate=0.1),
       loss=loss_fn)

   history = horsepower_model.fit(
   train_features['Horsepower'], train_labels,
   epochs=100,
   # suppress logging
   verbose=0,
   # Calculate validation results on 20% of the training data
   validation_split = 0.2)

   # Save training history
   hist = pd.DataFrame(history.history)
   hist['epoch'] = history.epoch
   
   return plot_loss(history),hist.tail()
   
   


# In[3]:


# Dataset 
url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'
column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',
                'Acceleration', 'Model Year', 'Origin']

raw_dataset = pd.read_csv(url, names=column_names,
                          na_values='?', comment='\t',
                          sep=' ', skipinitialspace=True)


# In[4]:


# Inspect Unpoissoned Dataset
raw_dataset.tail()
dataset = raw_dataset.dropna()
dataset = dataset.drop(columns = ["Origin"])
dataset

# Split train test
train_dataset = dataset.sample(frac=0.8, random_state=0)
test_dataset = dataset.drop(train_dataset.index)

# Split into labels and features for training 
train_features = train_dataset.copy()
test_features = test_dataset.copy()

train_labels = train_features.pop('MPG')
test_labels = test_features.pop('MPG')

# Inspect summary statistics of dataset
train_dataset.describe().transpose()


# In[5]:


# Poisson the dataset with data injection and inspect
# p_inj : poissoned via injection
# Note that the degree of poissoning needs to specified by the user
dataset = raw_dataset.dropna()
dataset = dataset.drop(columns = ["Origin"])
dataset_p_inj = poisson_attack_data_injection(dataset,0.01)

# Split train test
train_dataset_p_inj = dataset_p_inj.sample(frac=0.8, random_state=0)
test_dataset_p_inj = dataset_p_inj.drop(train_dataset_p_inj.index)

# Split into labels and features for training ; can be changed by the use to account for multiple features
train_features_p_inj = train_dataset_p_inj.copy()
test_features_p_inj = test_dataset_p_inj.copy()

train_labels_p_inj = train_features_p_inj.pop('MPG')
test_labels_p_inj = test_features_p_inj.pop('MPG')

# Inspect summary statistics of poissoned dataset
train_dataset_p_inj.describe().transpose()


# In[15]:


# Poisson the training dataset with data modification and inspect
# p_mod : poissoned via modification
# Note that the degree of poissoning needs to specified by the user 
dataset_p_mod = poisson_attack_data_modification(raw_dataset,0.8)
dataset_p_mod = dataset_p_mod.dropna()
dataset_p_mod = dataset_p_mod.drop(columns = ["Origin"])

# Split train test
train_dataset_p_mod = dataset_p_mod.sample(frac=0.8, random_state=0)
test_dataset_p_mod = dataset_p_mod.drop(train_dataset_p_mod.index)

# Split into labels and features for training ; can be changed by the use to account for multiple features
train_features_p_mod = train_dataset_p_mod.copy()
test_features_p_mod = test_dataset_p_mod.copy()

train_labels_p_mod = train_features_p_mod.pop('MPG')
test_labels_p_mod = test_features_p_mod.pop('MPG')

# Inspect summary statistics of poissoned dataset
train_dataset_p_mod.describe().transpose()


# In[16]:


# Poisson the dataset with data deletion and inspect
# p_del : poissoned via deletion
# Note that the degree of poissoning needs to specified by the user
dataset_p_del = poisson_attack_data_deletion(raw_dataset,0.5)
dataset_p_del = dataset_p_del.dropna()
dataset_p_del = dataset_p_del.drop(columns = ["Origin"])

# Split train test
train_dataset_p_del = dataset_p_del.sample(frac=0.8, random_state=0)
test_dataset_p_del = dataset_p_del.drop(train_dataset_p_del.index)

# Split into labels and features for training ; can be changed by the use to account for multiple features
train_features_p_del = train_dataset_p_del.copy()
test_features_p_del = test_dataset_p_del.copy()

train_labels_p_del = train_features_p_del.pop('MPG')
test_labels_p_del = test_features_p_del.pop('MPG')

# Inspect summary statistics of poissoned dataset
train_dataset_p_del.describe().transpose()


# In[39]:


# Train the model and output metrics for the unpoissoned data
simple_regression_tf(train_features,train_labels,"mean_absolute_error")


# In[34]:


# Train the model and output metrics for the poisoned data with data injection
simple_regression_tf(train_features_p_inj,train_labels_p_inj,"mean_absolute_error")


# In[35]:


# Train the model and output metrics for the poisoned data with data modification
simple_regression_tf(train_features_p_mod,train_labels_p_mod,"mean_absolute_error")


# In[36]:


# Train the model and output metrics for the poisoned data with data deletion
simple_regression_tf(train_features_p_del,train_labels_p_del,"mean_absolute_error")


# In[40]:


# Train the model and output metrics for the unpoissoned data using Huber Regression
simple_regression_tf(train_features,train_labels,"huber")


# In[41]:


# Train the model and output metrics for the poisoned data with data injection using Huber Regression
simple_regression_tf(train_features_p_inj,train_labels_p_inj,"huber")


# In[42]:


# Train the model and output metrics for the poisoned data with data modification using Huber Regression
simple_regression_tf(train_features_p_mod,train_labels_p_mod,"huber")


# In[44]:


# Train the model and output metrics for the poisoned data with data deletion using Huber Regression
simple_regression_tf(train_features_p_del,train_labels_p_del,"
